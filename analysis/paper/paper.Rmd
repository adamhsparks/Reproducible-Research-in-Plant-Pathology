---
title: "Open Science, Open Plant Pathology"
author:
  - Adam H. Sparks:
      email: Adam.Sparks@usq.edu.au
      institute: USQ
      correspondence: true
      equal_contributor: true
  - Emerson Del Ponte:
      institute: UFV
      equal_contributor: true
  - Kaique dos Santos Alves:
      institute: UFV
  - Zachary Foster: 
      institute: OSU
  - Niklaus J Grünwald: 
      institute: ARS
institute:
  - USQ: University of Southern Queensland, Centre for Crop Health, Toowoomba, Queensland, Australia
  - UFV: Departmento de Fitopatologia, Universidade Federal de Viçosa, Brazil
  - OSU: Department of Botany and Plant Pathology, Oregon State University, Corvallis, OR, USA
  - ARS: Horticultural Crops Research Unit, USDA Agricultural Research Service, Corvallis, OR, USA
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::word_document2:
      fig_caption: yes
      reference_docx: "../templates/template.docx" # Insert path for the DOCX file
      pandoc_args:
        - '--lua-filter=scholarly-metadata.lua'
        - '--lua-filter=author-info-blocks.lua'
bibliography: references.bib
csl: "../templates/phytopathology.csl" # Insert path for the bib-style
abstract: |
  Open research practices have been highlighted extensively during the last ten years in many fields of study as essential standards needed to promote transparency and reproducibility of scientific results. In fact, scientific claims can only be evaluated based on how protocols, materials, equipment and methods were described; data were collected and prepared; and analyses conducted. Openly sharing protocols, data and computational code are central for current scholarly dissemination and communication, but in many fields, including plant pathology, adoption of these practices has been slow. We randomly selected 300 articles published from 2012 to 2018 across 21 journals representative of the pathology discipline, and assigned them scores reflecting their openness and reproducibility. We found that most of the articles were not very open, failing to share data or code in a meaningful way. We propose that using open source tools for producing open and reproducible work and analysis is advantageous, benefiting not just readers, but the authors as well and provide ideas and tools to promote open, reproducible research practices among plant pathologists.
keywords: |
  open science; open data; reproducible research; FAIR data
highlights: |
  Open research practices are highlighted extensively in the broader scientific community. Our survey of 300 randomly selected articles from 2012 to 2018 indicates that we have work to do in this area as plant pathologists with most articles scoring low in openness and reproducibility.
---

```{r, setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/"
)

library(Reproducibility.in.Plant.Pathology)
```

# Introduction

Modern plant pathological research has many facets given the array of disciplines and sub-disciplines currently involved.
Collectively, they contribute to increase our basic and applied knowledge on several aspects of pathogen biology and disease development to ultimately improve plant disease management.
Scientific research in the field varies from the purely observational or descriptive nature to inferential based on experimental or simulation-derived data sets.
Whatever the case, research findings are verifiable based on how much of the research materials, processes and outcomes are made available beyond what is reported in the scientific article and the ability of others to make use of your methods and results.
These research findings include biological materials (isolates or strains), nucleic/protein sequences, experimental and simulated raw data annotations, drawings and photographs and statistical analysis code among other materials and data generated as a course of conducting research.
That is, open science leads to reproducibility and replicability.

## Definitions

In order for us to easily discuss the topic, we first must define what we mean so that we may clearly communicate.
Many of the terms used in this area have varying definitions that may or may not agree with each other. For instance, reproducible research is recently highlighted by many authors [@Baker2016a; @Iqbal2016; @Nature2016; @Patil2016; @Weissgerber2016; @Brunsdon2015; @Sweedler2015; @Fitzjohn2014; @Ioannidis2014; @Fidler2013; @Stodden2013] as an important issue.
The International Union of Pure and Applied Chemistry provides
definitions of repeatability in the "Compendium of Chemical Terminology -- Gold Book" as:

> "The closeness of agreement between independent results obtained with the same method on identical test material, under the same conditions (same operator, same apparatus, same laboratory and after short intervals of time). The measure of repeatability is the standard deviation qualified with the term: `repeatability' as repeatability standard deviation. In some contexts repeatability may be defined as the value below which the absolute difference between two single test results obtained under the above conditions, may be expected to lie with a specified probability", [@mcnaught1997compendium].

and reproducibility is defined as:

> "The closeness of agreement between independent results obtained with the same method on identical test material but under different conditions (different operators, different apparatus, different laboratories and/or after different intervals of time). The measure of reproducibility is the standard deviation qualified with the term 'reproducibility' as reproducibility standard deviation. In some contexts reproducibility may be defined as the value below which the absolute difference between two single test results on identical material obtained under the above conditions, may be expected to lie with a specified probability. Note that a complete statement of reproducibility requires specification of the experimental conditions which differ", [@mcnaught1997compendium].

However, while these definitions are clear and well accepted in physical sciences, in biological sciences it is not always possible to use identical test material or perhaps the the time or resources are not available for full reproduction, _e.g._, field trials that span years and locations or complex glasshouse experiments.
Therefore we will follow Peng's [-@Peng2009] definition that provides clear guidelines a minimum standard of ``reproducible research'':

> "The replication of scientific findings using independent investigators, methods, data, equipment, and protocols has long been, and will continue to be, the standard by which scientific claims are evaluated. However, in many fields of study there are examples of scientific investigations that cannot be fully replicated because of a lack of time or resources. In such a situation, there is a need for a minimum standard that can fill the void between full replication and nothing. One candidate for this minimum standard is ``reproducible research'', which requires that data sets and computer code be made available to others for verifying published results and conducting alternative analyses". Peng, R. [-@Peng2009]. _Reproducible research and Biostatistics_. Biostatistics, 10 (3): 405-408.

### Open Science

Open science has become highlighted lately with many donors expecting data to be available [@government_of_canada_2016; @vannoorden2017; @ARC2018] and other scientists interested in sharing and collaborating more widely [@Wald2010].

# A General Workflow

A general workflow for producing academic research involves clearly defining a research question, obtaining data for testing the hypothesis, summarizing/analyzing and presenting data and results, and writing the manuscript.
Here we defined three levels of reproducibility which are also related with the evolution of computational methods and reproducible practices \@ref(fig:workflow-dia).

A first level of reproducibility involves openly available research materials such as isolates or strains and/or nucleic acid sequences in public collections and citations for methods used.
A second level involves providing raw data and code as binary files (PDF or other non-text file) in supplemental materials, which do not allow prompt access to the data and running the code because of use of expensive commercial software or a pay-wall.
The highest level includes efforts to annotate structured raw data (FAIR -- Findable, Accessible, Interoperable and Re-usable) [@Wilkinson2016] and fully document the analysis using open source code which are deposited in public repositories and can be run by anyone following download of data and code.
The first level, as reported, is an essential step that is not substituted by the other practices and eventually researchers fail to provide sufficient description or correct citations.
In the next section we present standards and tools that can be used to ensure reproducibility.

### Methods

When making your science more open and reproducible, methods, software used, this includes items such as R or Python packages that were directly used in the analysis or production of the paper, etc. should be cited properly.
Deposit and annotate biological materials with an herbarium or other repository.
Provide a full description of all equipment used, _e.g._ a Spectrum Technologies Watchdog 2700 weather station was used to record wind speed, direction, rainfall, temperature and relative humidity at one hour intervals.
This allows end users to identify what was used and identify the methods used more accurately.
Just as importantly, this acknowledges the contributions of others whose works were instrumental in your research.
This also helps ensure that you as a researcher are able to reconstruct what you have done since you will have good notes and documentation and be able to identify if something changes, _e.g._ a package version, what effect it had on your research.

The use of programming or scripting languages such as R, SAS or Python enable you as a scientist to keep very detailed records of what was computationally performed.
This is as opposed to using software such as spreadsheet programs like Excel, gsheets, Numbers, Calc or others which can be used for simple statistical analyses and visualization (but should not be) or other point-and-click software packages that do not enable you to keep an accurate record of the steps taken to import, format, visualise and analyse data.

### Data

Prefer text files for saving small sets of data.
Data that are saved in binary formats such as PDF files are difficult to reuse because they are not easily machine readable.
In many cases, data sets are small enough and curated in spreadsheets, which should be saved as a plain text file, _e.g._ comma separated (CSV) files.
This also helps ensure that the data are reusable.Larger data sets may warrant the use of a proper database like MariaDB or PostgreSQL which provide users with several benefits but two important ones to mention here are data redundancy, no duplicated records; and data consistency, all of your records are sure to be recorded in the same way for every observation.
While databases may offer many advantages, the trade-off is that they are more complex to set up and administer, especially for a small data set that might be only a few rows and columns in a database.

Ideally, once the data are complete, best practices for keeping your data as you perform your work include treating the raw data as read-only and using file permissions to prevent changes to the raw data files.
It should be noted, that the use of a database management system also allows for both of these at the expense of added complexity.
Saving files in proprietary formats such as .xls(x) can also lead to issues in the future when opening using newer (or older) software versions.
Unexpected changes to values in the data [@Ziemann2016] may also occur when using proprietary formats.

If steps are followed to make the data FAIR then it will be readable by humans and machines alike this will help support new discoveries and support further research.
In turn sharing data will lead to new citations for your work as other discover and use it.

To make your data the most widely usable, ensure that it has a persistent identifier.
A digital object identifier (DOI) is the most common (<https://www.doi.org/>) but handle is also an option (<https://handle.net/>).
There are different options for generating a DOI for your data and other materials.
FigShare, Zenodo and OSF all offer long-term archival along with a service to generate a DOI for your materials.
The use of a persistent identifier works to ensure that even if the data are moved, they can still be located using that unique identifier.

For more on FAIR data, visit Go-Fair <https://www.go-fair.org/fair-principles/>.

### Sharing Your Research

Once you have determined how to best manage your source code for analysis and the data sets the next step is to consider how to share your data.
Providers such as FigShare, Dataverse, OSF.io and Zenodo allow for you to deposit your data and generate a DOI for sharing your project once you are finished with it.

Other providers exist that allow for you to not only track changes but also to share the data openly, these include GitHub, Gitlab and Bitbucket.
GitHub is arguably the most popular and widely used software development platform currently.
Data that is encoded in CSV or other plain text formats can easily be deposited in a repository along with code for analysis to enable changes to be tracked and other users to download and replicate the work.

We would advise against the practice of depositing data on a laboratory website or a site such as GitHub only though.
Doing either of these leaves the work in an unstable state where future users may be unable to access the work.
It is a best practice to always ensure that you have deposited the data with a provider such as Zenodo, FigShare or OSF and generated a DOI for the materials to help ensure continued accessibility.
Many of these providers provide rather easy ways to link the project with a software development repository to help ensure that the data are available in perpetuity.

Readers should also consult with their local librarians about local resources.
Most universities provide a facility for staff to deposit papers and other academic materials, but this may extend to software development repositories in some cases up as well.

Readers are encouraged to avoid using lab websites and other personal pages for sharing projects over the longer term as they are fraught with link-rot and are not an optimal way to share your projects.

# Status in Plant Pathology

## Article Selection

In order for us to determine where we are as a discipline with regards to openly and reproducibly sharing data, we randomly selected 300 articles published from 2012 to 2018 across 21 journals (Table \@ref(tab:journals)) representative of the pathology discipline, and assigned them scores reflecting their openness and reproducibility.
Among the journals, both fundamental and/or applied as well as journals covering specific group of pathogens/plants or broad areas were included.
Three hundred articles were randomly selected from issues published from 2012 to 2018.
A list of randomly selected pages was assigned to a randomized list of the 21 journals [@Sparks2017] where the page number fell within an article for the given journal.
In cases where an article was not suitable, _e.g._, a review or otherwise not related to plant pathology, the next article was selected until a suitable article was found.
Notes regarding the selection of articles can be found in the file, XXXX, available in this paper's repository.
The pages list was numbered from page one and went to 150.
This was done since some journals restart their numbering with each issue and also ensures that the journal is more likely to have a page number corresponding to the randomly generated value.
This also assumes that there is no effect or bias on reproducibility based on the time of year that an article was published, since most journals start with page number one at the beginning of the year.
The list of journals was saved from Google's gsheets as a comma separated value (CSV) file for later use and distribution.

## Scoring criteria

Four individuals were assigned to rate a randomised list of journal articles.

Journals were provided a score for whether they were completely open (TRUE), behind a pay wall (FALSE) or a combination (BOTH).
The five-year impact factor for 2018 for each journal was retrieved from InCites Journal Citation Reports, Clarivate Analytics and entered in a separate sheet in the Google sheets file.
This was downloaded, saved as a .csv file and is left-joined with the article notes using `dplyr::left_join()` for analysis when the `import_notes()` command is executed.

Articles were provided classifications, whether they were fundamental or applied and a note was made whether the article's primary focus was molecular (TRUE/FALSE).
This classification was based on the evaluator's own judgement of the article.

Further classification as to whether the articles focused on using or developing molecular techniques was included.

Where possible the software used was recorded in the notes if it was cited or otherwise specified in the article text.

Articles were scored in four areas for reproducibility on a scale of 0 to 3:

  * **computational methods available**

    * 0 - Not available or not mentioned in the publication

    * 1 - Uses expensive proprietary software that only institutions would typically purchase (_i.e._, 
    ArcGIS standard is $7000)

    * 2 - Uses proprietary software that most individuals can afford (_e.g._, Excel, Matlab? ($500))

    * 3 - Uses entirely open source and free software (_e.g._, R, Python)

    * NA - No software was used in the research that can be determined as the article is written

  * **software available**

    * 0 - Not available or not mentioned in the publication

    * 1 - Uses expensive proprietary software that only institutions would typically purchase (_i.e._, 
    ArcGIS standard is $7000)

    * 2 - Uses proprietary software that most individuals can afford (_e.g._, Excel, Matlab? ($500))

    * 3 - Uses entirely open source and free software (_e.g._, R, Python).

    * NA - No software was used in the research that can be determined as the article is written

  * **software cited**

    * 0 - not mentioned

    * 1 - Software mentioned by name only

    * 2 - Software cited with version number

    * 3 - All software components (SAS PROCs, R or Python packages) etc. cited

    * NA - No software was used in the research that can be determined as the article is written.
    
  * **data available**

    * 0 - Not available or not mentioned in the publication

    * 1 - Available upon request to author
  
    * 2 - Online, but inconvenient or non-permanent (_e.g._, login needed, pay wall, FTP server, personal 
    lab website that may disappear, or may have already disappeared)
    
    * 3 - Freely available online to anonymous users for foreseeable future (_e.g._, archived using Zenodo, 
    dataverse or university library or some other proper archiving system)

A custom R function, `Reproducibility.in.Plant.Pathology::import_notes()`, was written to import the data, format the columns properly and calculate the overall reproducibility score in R [@RCT2020].

## Results

No journals were completely closed, all offered at least an option for open access but some were completely open.

The majority of the articles were classed as fundamental, 160, with 138 classed as applied, nearly an even distribution between the types of article.

Most articles did not make any computational methods available in any fashion with two classing as "3" (0.007&nbsp;%), which was the highest score available, 287 were classed as "0" (96&nbsp;%) and 9 articles (0.03&nbsp;%) appeared to not use any computational methods.

More articles did a good job of using software that was reasonably available to anyone with 206 scoring "2" or "3" (Table \@ref(tab:sa)).

The majority of articles scored very low in our reproducibility assessment, 82.6&nbsp;% scored 0 out of 12, which was not reproducible at all (Table \@ref(tab:reproducibility)).
The highest scoring article received an 11 out of 12.

* Madden et al. [-@Madden2015] supply an *e-**X**tra*\* with reproducible examples
for readers.
* Duku et al. [-@Duku2016] provide models, data and code, (http://adamhsparks.github.io/MICCORDEA/) necessary to
replicate the entire study modelling the effects of climate change on rice bacterial blight and rice leaf blast in Tanzania.
* Sparks et al. [-@Sparks2011; -@Sparks2014] provide models, data and code, (http://adamhsparks.github.io/Global-Late-Blight-MetaModelling/)
necessary to replicate model development and the subsequent the study on the effects of climate change on potato late blight.
* Del Ponte provides data and a reproducible report that explain in details all steps of the analysis and the R codes for conducting a meta-analysis for assessing heterogeneity in relationship between white mold incidence and soybean yield and between incidence and soybean tied.
* Example from Grünwald lab: 
  - paper http://apsjournals.apsnet.org/doi/full/10.1094/PHYTO-12-14-0350-FI
  - github repo https://github.com/grunwaldlab/Sudden_Oak_Death_in_Oregon_Forests
* Other examples from plant pathology providing e-Xtras or supplemental material

# Discussion

# Acknowledgements

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# References 
<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->
<div id="refs"></div>

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# Tables

```{r journals, echo=FALSE, message=FALSE, warning=FALSE}
library("dplyr")
library("janitor")
library("flextable")
library("officer")

import_notes() %>%
  tabyl(journal) %>%
  select(-percent) %>%
  flextable() %>%
  set_caption(
    "Table: (\\#tab:journals) Journal titles selected for inclusion and the number (n) of articles from each journal that were evaluated."
  ) %>% 
  autofit()
```

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

```{r software_avail, echo=FALSE, message=FALSE, warning=FALSE}
import_notes() %>%
  tabyl(software_avail) %>%
  select(-percent) %>% 
  rename(`software availability` = software_avail,
         percent = valid_percent) %>% 
  flextable() %>%
  set_caption(
    "Table: (\\#tab:sa) Article ratings for software availability where '0', not available or not mentioned in publication to '3', preferably open source and freely available."
  ) %>% 
  autofit()
```

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

```{r reproducibility_score, echo=FALSE, message=FALSE, warning=FALSE}
import_notes() %>% 
  tabyl(reproducibility_score) %>% 
  adorn_pct_formatting() %>% 
  rename(score = reproducibility_score) %>% 
  flextable() %>% 
  set_caption("Table: (\\#tab:reproducibility) Tally of reproducibility scores where 0 is not reproducible and 12 is fully reproducible, _e.g._ all software cited, software used are freely available to anyone, computational methods (statistical analysis or other computational code) are freely available for download and use, and raw data are available for download and use.") %>% 
    autofit()

```

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# Figures

```{r workflow-dia, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='An open and reproducible research workflow.', cache=FALSE, fig.env='figure'}
library(DiagrammeR)
library(DiagrammeRsvg)
library(rsvg)

p <- export_svg(
  grViz(
    "digraph Fig1 {
    graph [fontsize = 8, nodesep=0.5]
  
    node [shape = box, style = filled, fillcolor = grey99, width = 1.2,fontname = Helvetica]
    Question; Methodology; Data; Analysis; Manuscript;
  
    node [shape = box, style = filled, fillcolor = grey90, width = 1.2,fontname = Helvetica]
    Description; Availability; Citation
  
    node [shape = box, style = filled, fillcolor = grey80, width = 1.2]
    'Binary Code'; 'Binary File'; Supplement
  
    node [shape = box, style = filled, fillcolor = grey70, width = 1.2]
    'Source Code'; 'Data File'; 'Public Repository';
  
    Question->Methodology->Data->Analysis
    Analysis->Manuscript
    Methodology->Description
    Methodology->Citation
    Methodology->Availability
    Manuscript->Supplement
    Data->'Data File'->'Public Repository'
    Analysis->'Binary Code'->Supplement
    Analysis->'Source Code'->'Public Repository'
    Data->'Binary File'->Supplement
  }",
  width = 1000,
  height = 750
))
rsvg_pdf(charToRaw(p), "Figure 1.pdf")

knitr::include_graphics("Figure 1.pdf")
```

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, cache = FALSE}
# which R packages and versions?
sessionInfo()
```

The current Git commit details are:

```{r}
# what commit is this file at? You may need to change the path value
# if your Rmd is not in analysis/paper/
git2r::repository("../..")
```
